{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.datasets import fetch_mldata\n",
    "import random \n",
    "import math \n",
    "import pandas as pd\n",
    "import xlrd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "target = np.zeros((iris['target'].shape))\n",
    "for i in range (len(iris['target'])):\n",
    "    if iris['target'][i] == 0:\n",
    "        target[i] = 1\n",
    "    else :\n",
    "        target[i] = 0 \n",
    "features = iris['data'][:,[0,3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_func(x,theta):\n",
    "    num = 1\n",
    "    mult = x.dot(theta)\n",
    "    den = 1 + math.exp(-mult)\n",
    "    return num/den\n",
    "\n",
    "def log_reg(X,Y,iterations = 10, alpha = 0.1):\n",
    "    r,c = X.shape\n",
    "    x = np.c_[np.ones(r),X]\n",
    "    r,c = x.shape\n",
    "    theta = np.zeros((c,1))\n",
    "    for i in range (iterations):\n",
    "        for k in range(c):\n",
    "            derivative = 0 \n",
    "            for j in range(0,r):\n",
    "                derivative += (sigmoid_func(x[j,:],theta) - Y[j])*x[j,k]\n",
    "            theta[k,0] = theta[k,0] - alpha*derivative\n",
    "    return theta\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r,c = features.shape\n",
    "x = np.c_[np.ones(r),features]\n",
    "theta = log_reg(features,target)\n",
    "predictions = np.zeros((target.shape))\n",
    "for i in range (target.shape[0]):\n",
    "    pred = sigmoid_func(x[i,:],theta)\n",
    "    if pred>=0.5:\n",
    "        predictions[i] = 1\n",
    "    else :\n",
    "        predictions[i] = 0\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X,train_Y = shuffle(features,target,random_state = 42)\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_X,train_Y)\n",
    "clf.predict(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEURAL NETWORKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    def __init__(self,x,y,hiddenlayers,activations,epochs,alpha):\n",
    "        self.input = x\n",
    "        self.y = y\n",
    "        self.activations = activations\n",
    "        self.epochs = epochs\n",
    "        self.alpha = alpha\n",
    "        self.weights= {} \n",
    "        self.weights['1'] = np.random.rand(self.input.shape[1],hiddenlayers[0])\n",
    "        self.z_layers ={}\n",
    "        self.der_lin = None\n",
    "        self.der_relu = None\n",
    "        self.der_sig = None\n",
    "        #self.der_act = \n",
    "        for i in range (len(hiddenlayers)-1):\n",
    "            self.weights[str(i+2)] = np.random.rand(hiddenlayers[i],hiddenlayers[i+1])\n",
    "        self.output = np.zeros((self.y.shape))\n",
    "       \n",
    "        def der_relu(x):\n",
    "            row,col = np.shape(x)\n",
    "            t = np.copy(x)\n",
    "            for i in range(row):\n",
    "                for j in range(col):\n",
    "                    if t[i,j]>0:\n",
    "                        t[i,j]=1\n",
    "                    else:\n",
    "                        t[i,j]=0\n",
    "            return t\n",
    "        self.der_relu = der_relu\n",
    "       \n",
    "        def der_sig(x):\n",
    "            return (1./(1+np.exp(-x)))*(1-(1./(1+np.exp(-x))))\n",
    "        self.der_sig = der_sig\n",
    "       \n",
    "        def der_lin(x):\n",
    "            return np.ones(x.shape)\n",
    "        self.der_lin = der_lin\n",
    "        self.der_act = {'relu': self.der_relu, 'sigmoid' : self.der_sig, 'linear' : self.der_lin}\n",
    "            \n",
    "    def sigmoid(self,x):\n",
    "        return 1./(1+np.exp(-x))\n",
    "    \n",
    "    def ReLU(self,x):\n",
    "        return np.maximum(0,x)\n",
    "    \n",
    "    def act_layer(self,i,x):\n",
    "        if self.activations[i-1]== 'relu':\n",
    "            return self.ReLU(x)\n",
    "        elif self.activations[i-1] == 'sigmoid':\n",
    "            return self.sigmoid(x)\n",
    "        else :\n",
    "            return x\n",
    "    \n",
    "    def feedforward(self):\n",
    "        self.act_layers = {}\n",
    "        self.z_layers = {}\n",
    "        self.act_layers['1'] = self.act_layer(1,np.matmul(self.input,self.weights['1']))\n",
    "        self.z_layers['1'] = np.dot(self.input,self.weights['1'])\n",
    "        for k in range (len(self.activations)-1):\n",
    "            self.z_layers[str(k+2)] =  np.dot(self.act_layers[str(k+1)],self.weights[str(k+2)])\n",
    "            self.act_layers[str(k+2)] = self.act_layer(k+1,self.z_layers[str(k+2)])\n",
    "        self.output = self.act_layers[str(len(self.activations))]\n",
    "        return self.output\n",
    "    \n",
    "    def backpropagation(self):\n",
    "        self.dw = {}\n",
    "        self.errors = {}\n",
    "        self.errors[str(len(self.activations))] = (self.y - self.output)* self.der_act[self.activations[-1]](self.z_layers[str(len(self.activations))])\n",
    "        for i in reversed(range(len(self.activations)-1)):\n",
    "            \n",
    "            self.errors[str(i+1)] = (self.errors[str(i+2)].dot((self.weights[str(i+2)]).T))*(self.der_act[self.activations[i]](self.z_layers[str(i+1)]))\n",
    "        self.dw['1'] = (self.input.T).dot(self.errors['1'])\n",
    "        for i in range(len(self.weights)-1):\n",
    "            self.dw[str(i+2)] = (self.act_layers[str(i+1)].T).dot(self.errors[str(i+2)])\n",
    "        return(self.z_layers['1'].shape)                                                          \n",
    "  \n",
    "    \n",
    "    \n",
    "    def train(self):\n",
    "        for _ in range (self.epochs):\n",
    "            nn.feedforward()\n",
    "            nn.backpropagation()\n",
    "            mse_loss = (np.sum((self.y - self.output)*(self.y-self.output)))/len(self.y)\n",
    "            abs_loss = (np.sum((self.y - self.output)))/len(self.y)\n",
    "            for i in range(len(self.weights)):\n",
    "                self.weights[str(i+1)] = self.weights[str(i+1)] - (self.alpha)*(self.dw[str(i+1)]) \n",
    "            print('rmse loss :' ,math.sqrt(mse_loss))\n",
    "            print( 'abs_loss :', abs(abs_loss))\n",
    "    \n",
    "    def test(self,test_X,test_Y):\n",
    "        act_layers = {}\n",
    "        z_layers = {}\n",
    "        act_layers['1'] = self.act_layer(1,np.dot(test_X,self.weights['1']))\n",
    "        z_layers['1'] = np.dot(test_X,self.weights['1'])\n",
    "        for k in range (len(self.activations)-1):\n",
    "            z_layers[str(k+2)] =  np.dot(act_layers[str(k+1)],self.weights[str(k+2)])\n",
    "            act_layers[str(k+2)] = self.act_layer(k+2,z_layers[str(k+2)])\n",
    "        output = act_layers[str(len(self.activations))]\n",
    "        print(z_layers['2'].shape)\n",
    "        mse_loss = (np.sum((test_Y - output)*(test_Y - output)))/len(test_Y)\n",
    "        abs_loss = (np.sum((test_Y - output)))/len(test_Y)\n",
    "        print('rmse loss :' ,math.sqrt(mse_loss))\n",
    "        print( 'abs_loss :', abs(abs_loss))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_est_price1=pd.read_excel('Real_estate.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_est_price=r_est_price1.loc[:,['X1 transaction date', 'X2 house age',\n",
    "       'X3 distance to the nearest MRT station',\n",
    "       'X4 number of convenience stores', 'X5 latitude', 'X6 longitude',\n",
    "       'Y house price of unit area']]\n",
    "r_est_price.columns=['X1', 'X2',\n",
    "       'X3',\n",
    "       'X4', 'X5', 'X6',\n",
    "       'Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse loss : 40.218465126028875\n",
      "abs_loss : 37.76225406320998\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n",
      "rmse loss : 40.387157505195646\n",
      "abs_loss : 37.94186851211073\n"
     ]
    }
   ],
   "source": [
    "Y = r_est_price.values[:,6].reshape(-1,1)\n",
    "X = r_est_price.values[:,0:5]\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_X,test_X,train_Y,test_Y = train_test_split(X,Y,test_size = 0.3,random_state = 42)\n",
    "nn = NeuralNetwork(train_X,train_Y,[4,2,1],['sigmoid','sigmoid','linear'],100,0.1)\n",
    "nn.feedforward()\n",
    "nn.backpropagation()\n",
    "nn.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(125, 2)\n",
      "rmse loss : 39.11240110246366\n",
      "abs_loss : 36.91280000000001\n"
     ]
    }
   ],
   "source": [
    "nn.test(test_X,test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:77: DeprecationWarning: Function fetch_mldata is deprecated; fetch_mldata was deprecated in version 0.20 and will be removed in version 0.22\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:77: DeprecationWarning: Function mldata_filename is deprecated; mldata_filename was deprecated in version 0.20 and will be removed in version 0.22\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "mnist = fetch_mldata('MNIST original')\n",
    "X = mnist.data\n",
    "Y = mnist.target\n",
    "train_X,test_X,train_Y,test_Y = train_test_split(X,Y,test_size = 0.3,random_state = 42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork_clf():\n",
    "    def __init__(self,x,y,hiddenlayers,activations,epochs,alpha):\n",
    "        self.input = x\n",
    "        self.y = y\n",
    "        self.activations = activations\n",
    "        self.epochs = epochs\n",
    "        self.alpha = alpha\n",
    "        self.weights= {} \n",
    "        self.weights['1'] = np.random.rand(self.input.shape[1],hiddenlayers[0])\n",
    "        self.z_layers ={}\n",
    "        self.der_lin = None\n",
    "        self.der_relu = None\n",
    "        self.der_sig = None\n",
    "        #self.der_act = \n",
    "        for i in range (len(hiddenlayers)-1):\n",
    "            self.weights[str(i+2)] = np.random.rand(hiddenlayers[i],hiddenlayers[i+1])\n",
    "        self.output = np.zeros((self.y.shape))\n",
    "       \n",
    "        def der_relu(x):\n",
    "            row,col = np.shape(x)\n",
    "            t = np.copy(x)\n",
    "            for i in range(row):\n",
    "                for j in range(col):\n",
    "                    if t[i,j]>0:\n",
    "                        t[i,j]=1\n",
    "                    else:\n",
    "                        t[i,j]=0\n",
    "            return t\n",
    "        self.der_relu = der_relu\n",
    "       \n",
    "        def der_sig(x):\n",
    "            return (1./(1+np.exp(-x)))*(1-(1./(1+np.exp(-x))))\n",
    "        self.der_sig = der_sig\n",
    "       \n",
    "        def der_lin(x):\n",
    "            return np.ones(x.shape)\n",
    "        self.der_lin = der_lin\n",
    "        self.der_act = {'relu': self.der_relu, 'sigmoid' : self.der_sig, 'linear' : self.der_lin}\n",
    "            \n",
    "    def sigmoid(self,x):\n",
    "        return 1./(1+np.exp(-x))\n",
    "    \n",
    "    def ReLU(self,x):\n",
    "        return np.maximum(0,x)\n",
    "    \n",
    "    def act_layer(self,i,x):\n",
    "        if self.activations[i-1]== 'relu':\n",
    "            return self.ReLU(x)\n",
    "        elif self.activations[i-1] == 'sigmoid':\n",
    "            return self.sigmoid(x)\n",
    "        else :\n",
    "            return x\n",
    "    \n",
    "    def feedforward(self):\n",
    "        self.act_layers = {}\n",
    "        self.z_layers = {}\n",
    "        self.act_layers['1'] = self.act_layer(1,np.dot(self.input,self.weights['1']))\n",
    "        self.z_layers['1'] = np.dot(self.input,self.weights['1'])\n",
    "        for k in range (len(self.activations)-1):\n",
    "            self.z_layers[str(k+2)] =  np.dot(self.act_layers[str(k+1)],self.weights[str(k+2)])\n",
    "            self.act_layers[str(k+2)] = self.act_layer(k+1,self.z_layers[str(k+2)])\n",
    "        self.output = self.act_layers[str(len(self.activations))]\n",
    "        return self.output\n",
    "    \n",
    "    def backpropagation(self):\n",
    "        self.dw = {}\n",
    "        self.errors = {}\n",
    "        self.errors[str(len(self.activations))] = (-(self.y/self.output)+(1-self.y)/(1-self.output))* self.der_act[self.activations[-1]](self.z_layers[str(len(self.activations))])\n",
    "        for i in reversed(range(len(self.activations)-1)):\n",
    "            \n",
    "            self.errors[str(i+1)] = (self.errors[str(i+2)].dot((self.weights[str(i+2)]).T))*(self.der_act[self.activations[i]](self.z_layers[str(i+1)]))\n",
    "        self.dw['1'] = (self.input.T).dot(self.errors['1'])\n",
    "        for i in range(len(self.weights)-1):\n",
    "            self.dw[str(i+2)] = (self.act_layers[str(i+1)].T).dot(self.errors[str(i+2)])\n",
    "        return(self.z_layers['1'].shape)                                                          \n",
    "  \n",
    "    \n",
    "    \n",
    "    def train(self):\n",
    "        for _ in range (self.epochs):\n",
    "            nn.feedforward()\n",
    "            nn.backpropagation()\n",
    "            mse_loss = (np.sum((self.y - self.output)*(self.y-self.output)))/len(self.y)\n",
    "            abs_loss = (np.sum((self.y - self.output)))/len(self.y)\n",
    "            for i in range(len(self.weights)):\n",
    "                self.weights[str(i+1)] = self.weights[str(i+1)] - (self.alpha)*(self.dw[str(i+1)]) \n",
    "            print('rmse loss :' ,math.sqrt(mse_loss))\n",
    "            print( 'abs_loss :', abs(abs_loss))\n",
    "    \n",
    "    def test(self,test_X,test_Y):\n",
    "        act_layers = {}\n",
    "        z_layers = {}\n",
    "        act_layers['1'] = self.act_layer(1,np.dot(test_X,self.weights['1']))\n",
    "        z_layers['1'] = np.dot(test_X,self.weights['1'])\n",
    "        for k in range (len(self.activations)-1):\n",
    "            z_layers[str(k+2)] =  np.dot(act_layers[str(k+1)],self.weights[str(k+2)])\n",
    "            act_layers[str(k+2)] = self.act_layer(k+1,self.z_layers[str(k+2)])\n",
    "        output = self.act_layers[str(len(self.activations))]\n",
    "        mse_loss = (np.sum((test_Y - output)*(test_Y - output)))/len(test_Y)\n",
    "        abs_loss = (np.sum((test_Y - output)))/len(test_Y)\n",
    "        print('rmse loss :' ,math.sqrt(mse_loss))\n",
    "        print( 'abs_loss :', abs(abs_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-7a2b20766fb5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNeuralNetwork_clf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_Y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'linear'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeedforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackpropagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#nn.train()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#nn.test(test_X,test_Y)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-ff9bbe0f6ce4>\u001b[0m in \u001b[0;36mbackpropagation\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mder_act\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork_clf(train_X,train_Y,[4,2,1],['sigmoid','sigmoid','linear'],100,0.1)\n",
    "nn.feedforward()\n",
    "nn.backpropagation()\n",
    "#nn.train()\n",
    "#nn.test(test_X,test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reference:https://medium.com/@a.mirzaei69/implement-a-neural-network-from-scratch-with-python-numpy-backpropagation-e82b70caa9bb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
